# -*- coding: utf-8 -*-
"""Bike_Sharing_Demand_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NFe_zRT-eCXV_Q7bPUqPwOXJfkwd4dHn

#**Importing Required Libraries**
"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
sns.set_style("ticks")
sns.set_context("poster");
import plotly.express as px
from scipy.stats import norm
import pandas as pd
import numpy as np
import tensorflow as tf
from datetime import datetime
import calendar
from sklearn.preprocessing import StandardScaler,MinMaxScaler
from sklearn.preprocessing import  LabelEncoder
from sklearn.linear_model import Lasso, Ridge, LinearRegression, ElasticNet
from sklearn.tree import DecisionTreeRegressor, ExtraTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn import neighbors
from lightgbm import LGBMRegressor
import lightgbm
from xgboost import XGBRegressor
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
from sklearn import metrics
from sklearn.metrics import r2_score as r2
from sklearn.metrics import mean_squared_error as mse
from sklearn.metrics import mean_absolute_error
pd.options.display.max_rows = 50
pd.options.display.float_format = "{:.3f}".format
import warnings
warnings.filterwarnings('ignore')

"""# **Loading Data**

"""

b_data = pd.read_csv("data.csv", encoding ="ISO-8859-1")

"""#**Studying Data**"""

b_data.head()

b_data.describe().T

b_data.rename({"Temperature(°C)": "Temperature",
               "Functioning Day":"Functioning_Day",
                "Humidity(%)": "Humidity",
                "Wind speed (m/s)": "Wind_speed",
                "Visibility (10m)": "Visibility",
                "Dew point temperature(°C)": "Dew_point_temperature",
                "Solar Radiation (MJ/m2)": "Solar_Radiation",
                "Snowfall (cm)": "Snowfall",
                "Rainfall(mm)": "Rainfall",
                "Rented Bike Count": "Rented_Bike_Count"},
                axis = "columns", inplace = True)

b_data.isnull().sum()

b_data.duplicated().value_counts()

df = b_data.copy()

df['Date'] = pd.to_datetime(df['Date'], infer_datetime_format=True, dayfirst=True)

df['month'] = pd.DatetimeIndex(df['Date']).month
df['month'] = df['month'].apply(lambda x: calendar.month_abbr[x])

df['day'] = df['Date'].dt.day_name()

df['year'] = df['Date'].dt.year

df.head(1)

def total(df,var):
  total = len(df[var].value_counts())
  return total

total_lenght_different_column = {
    'Seasons': total(df,'Seasons'),
    'Holiday': total(df,'Holiday'),
    'Funtioning_Day': total(df,'Functioning_Day'),
    'month' : total(df,'month'),
    'day'   : total(df,'day'),
    'year'  : total(df,'year')
}

total_df = pd.DataFrame.from_dict(total_lenght_different_column,orient='index')

df.drop(columns=['Date'],inplace=True)
df.head(1)

df['Hour']=df['Hour'].astype('object')

total_df.T

df['year'] = df['year'].astype('object')

df.info()

"""# **Exploratory Data Analysis**

"""

dfr = df.copy()

num_feature = dfr.select_dtypes(exclude='object')
print(f'Numerical feature : {num_feature.columns.to_list()}')

cat_feature = dfr.loc[:,  ~dfr.columns.isin(num_feature.columns.to_list())]
print(f'Categorical feature : {cat_feature.columns.to_list()}')

def density_plot(dfr,num_feature):
  graph = plt.figure(figsize = (20,30))
  for i,col in enumerate(num_feature) :
    sns.set_context('poster');
    plt.subplot(6,2,i+1);
    sns.distplot(dfr[col], color = '#055E85', fit = norm);
    feature = dfr[col]
    plt.axvline(feature.mean(), color='#ff033e', linestyle='dashed', linewidth=3,label= 'mean');
    plt.axvline(feature.median(), color='#A020F0', linestyle='dashed', linewidth=3,label='median');
    plt.title(f'{col.title()}');
    plt.tight_layout();

density_plot(dfr,num_feature)

for i in dfr.columns:
  plt.figure(figsize=(15,6))
  if i == 'Rented_Bike_Count':
    pass
  elif i in ['Seasons','Holiday','Functioning_Day','month','year']:
    grp = dfr.groupby([i], as_index = False)['Rented_Bike_Count'].sum().sort_values('Rented_Bike_Count',
                                                                                    ascending = False)
    sns.set_context('poster');
    sns.barplot(x=grp[i], y=grp["Rented_Bike_Count"]);
    plt.title(f"Rented_Bike_Count for each {i} with respect of Hour");
    print('\n')
  elif i == 'day':
    workingdays = {'day':['Monday','Tuesday','Wednesday','Thursday','Friday',	'Saturday',	'Sunday']}
    workingday = pd.DataFrame(workingdays)
    grp = dfr.groupby([i], as_index = False)['Rented_Bike_Count'].sum().sort_values('Rented_Bike_Count',
                                                                                    ascending = False)
    chart = workingday.merge(grp)
    sns.set_context('poster');
    sns.lineplot(data= chart, x=chart[i],y= chart['Rented_Bike_Count'], marker= 'o', color = 'green');
    plt.xticks(fontsize = 14, rotation = 90);
    plt.title(f"Rented_Bike_Count for each {i} with respect of Hour");
  else :
    grp = dfr.groupby([i], as_index = False)['Rented_Bike_Count'].sum().sort_values('Rented_Bike_Count',
                                                                                    ascending = False).head(20)
    sns.set_context('notebook');
    sns.pointplot(x=grp[i], y=grp["Rented_Bike_Count"]);
    plt.title(f"Rented_Bike_Count for {i} with respect of Hour");
    print('\n')

  plt.show()

def group_by(df,feature):
  data = df.groupby([feature], as_index = False)['Rented_Bike_Count'].sum().sort_values('Rented_Bike_Count', ascending = False)
  return data

group_by(dfr,'Functioning_Day')

group_by(dfr,'Holiday')

group_by(dfr,'year')

group_by(dfr,'Seasons')

weekend = group_by(dfr,'day')
chutti = weekend[(weekend['day'] == 'Saturday') | (weekend['day'] == 'Sunday')]
chutti

workingday = group_by(dfr,'day')
office = workingday[(workingday['day'] != 'Saturday') & (workingday['day'] != 'Sunday')]
office

d = office['Rented_Bike_Count'].sum() > chutti['Rented_Bike_Count'].sum()
print(f' Is it true that bike rented count is more on working day? {d}')

copy_for_future = dfr.copy()
trial = dfr.copy()

trial['Functioning_Day']= LabelEncoder().fit_transform(trial['Functioning_Day'])

trial['Functioning_Day'].value_counts()

corr = trial[['Rented_Bike_Count','Functioning_Day']].corr()
corr

plt.figure(figsize = (15,5));
sns.scatterplot(data = trial[['Rented_Bike_Count','Hour','Functioning_Day']], x = trial['Hour'], y = trial['Rented_Bike_Count'], hue = trial['Functioning_Day']);

value = dfr.drop(dfr[dfr['Functioning_Day'] == 'No'].index)

value['Functioning_Day'].value_counts()

value = value.drop(['Functioning_Day'], axis = 1)
value.shape

print(f'Shape of original data: {copy_for_future.shape}')
print(f'Shape of new data : {value.shape}')

value['week'] = value['day'].apply(lambda x:'Weekend'  if x=='Saturday' or  x== 'Sunday' else 'workingdays')

value.week.value_counts()

"""# **Encoding Categorial Variable**"""

values = value.drop(columns=['day'], axis = 1)

values = values.drop(columns =['year'], axis = 1)

values = values.drop(columns=['Dew_point_temperature'], axis = 1)

num = values.select_dtypes(exclude ='object')
cat = values.select_dtypes(include ='object')
print(f' numeric: {num.columns.to_list()}\n categorial : {cat.columns.to_list()}')

encoded = values.apply(LabelEncoder().fit_transform)
encoded.head(1)

dumcoded = pd.get_dummies(values,drop_first=True,sparse=True)
dumcoded.head(1)

"""* When encoded using dummies, column size is increased to 47. Therefore, using label encoded values in futher process, however there is no issue in using get_dummies but sometime it may led to [dummy variable trap](https://www.geeksforgeeks.org/ml-dummy-variable-trap-in-regression-models/) which can again cause multicollinearity.

#**Gausssian Transformation**
"""

fig,axes = plt.subplots(1,3,figsize=(20,5))
sns.distplot(np.log1p(encoded['Rented_Bike_Count']),ax=axes[0],color='red').set_title("log 10");
sns.distplot(np.cbrt(encoded['Rented_Bike_Count']),ax=axes[1],color='blue').set_title("Cube root");
sns.distplot(np.sqrt(encoded['Rented_Bike_Count']),ax=axes[2], color='green').set_title("Square root");

encoded['Rented_Bike_Count']=np.sqrt(encoded['Rented_Bike_Count'])

"""#**Model Training**"""

X=encoded.drop('Rented_Bike_Count',axis=1)
y=encoded['Rented_Bike_Count']

X.head(1)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 7)

print(f'Shape of X_train => {X_train.shape}, Shape of X_test => {X_test.shape}' )
print(f'Shape of y_train => {y_train.shape}, Shape of y_test => {y_test.shape}' )

scaler = StandardScaler()

X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

model1 = [
          ['Linear Regression ', LinearRegression()],
           ['Lasso ', Lasso(alpha =0.1 , max_iter= 2000)],
           ['Ridge ', Ridge(alpha =0.1 , max_iter= 2000)],
           ['KNeighborsRegressor ',  neighbors.KNeighborsRegressor()],
           ['RandomForest ',RandomForestRegressor(criterion='absolute_error',random_state=42)]
        ]

model_score = []
for name,model in model1 :
    model_data = {}
    model_data["Name"] = name
    model.fit(X_train,y_train)
    y_pred = model.predict(X_test)
    model_data["MSE"] = mse(y_test, y_pred)
    model_data["RMSE"] = np.sqrt(model_data["MSE"])
    model_data["R2_Score"] = r2(y_test, y_pred)
    model_data["ADJ_R2"] = 1-(1-r2(y_test, y_pred))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1))
    print(f'    {20* "="}  {name} {20* "="}')
    plt.figure(figsize=(9,5))
    plt.plot((y_pred)[:100])
    plt.plot((np.array(y_test)[:100]))
    plt.legend(["Predicted","Actual"])
    plt.show()
    model_score.append(model_data)

model1_df = pd.DataFrame(model_score)
model1_df

model1_df.sort_values(by=['R2_Score'], ascending = False)

model2 = [
           ['GradientBoostingRegressor ', GradientBoostingRegressor(n_estimators=400, max_depth=4)] ,
           ['Light-GBM ', lightgbm.LGBMRegressor(num_leaves=41, n_estimators=400,random_state=42)],
           ['XGBRegressor ', XGBRegressor(objective= 'reg:squarederror')]
]

model_score2 = []
for name,model in model2 :
    model_data = {}
    model_data["Name"] = name
    model.fit(X_train,y_train)
    y_pred = model.predict(X_test)
    model_data["MSE"] = mse(y_test, y_pred)
    model_data["RMSE"] = np.sqrt(model_data["MSE"])
    model_data["R2_Score"] = r2(y_test, y_pred)
    model_data["ADJ_R2"] = 1-(1-r2(y_test,y_pred))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1))
    print(f'    {20* "="}  {name} {20* "="}')
    plt.figure(figsize=(9,5))
    plt.plot((y_pred)[:100])
    plt.plot((np.array(y_test)[:100]))
    plt.legend(["Predicted","Actual"])
    plt.show()
    model_score2.append(model_data)

model2_df = pd.DataFrame(model_score2)
model2_df

a= [model1_df,model2_df]
final = pd.concat(a,ignore_index=True)
final.sort_values('R2_Score', ascending =False)

model = XGBRegressor(objective= 'reg:squarederror',subsample=0.7)

params = {
     'gamma' : [ -0.5, 0, 0.5, 1],
    'learning_rate' :[0.001, 0.01, 0.1, 5,10 ],
    'n_estimators' : [25,50,75,100,500] ,
    'max_depth' : [3, 5, 7,10]
        }

gridsearch = GridSearchCV(model , params , cv=2 , return_train_score=True)

gridsearch.fit(X_train , y_train )

print(gridsearch.best_params_)

gridsearch_predictions = gridsearch.predict( X_test )

XGBRegressor_with_hyper = {
'Name': "XGBRegressor_with_hyper",
'MSE': [mse(y_test,gridsearch.predict(X_test))],
'RMSE': [np.sqrt(mse(y_test,gridsearch.predict(X_test)))],
'R2_Score': [r2(y_test,gridsearch_predictions)],
'ADJ_R2': [1-(1-r2(y_test,y_pred))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1))],
}

Result = pd.DataFrame.from_dict(XGBRegressor_with_hyper)

b = [final,Result]
final_df = pd.concat(b,ignore_index=True)
final_df.sort_values('R2_Score', ascending =False)

y_pred_XGB = gridsearch.predict( X_test )

plt.figure(figsize=(8,5))
plt.plot((y_pred_XGB)[:50])
plt.plot((np.array(y_test))[:50])
plt.legend(["Predicted","Actual"])
plt.show()

pd.DataFrame(zip(y_test, y_pred_XGB, (y_test - y_pred_XGB)), columns = ['actual', 'predicted', 'Difference(act-pre)'])[:10]

rf_optimal_model = gridsearch.best_estimator_

rf_optimal_model.feature_importances_

t = pd.DataFrame(X_train, columns = X.columns)

importances = rf_optimal_model.feature_importances_

importance_dict = {'Feature' : list(t.columns),
                   'Feature Importance' : importances}

importance_df = pd.DataFrame(importance_dict)

importance_df.sort_values(by=['Feature Importance'],ascending=False)

fig = px.pie(importance_df, values='Feature Importance', names='Feature', title='Feature Importance');

fig.update_traces(textposition='inside', textinfo='percent+label');
fig.show();